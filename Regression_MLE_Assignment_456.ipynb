{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. MLE for Linear Regression\n",
    "\n",
    "* In linear regression the trick that we do is, we take the model that we need to find, as the mean of the above stated normal distribution. Because we know how to find MLE values of a mean in a normal distribution.\n",
    "\n",
    "\n",
    "Maximum Likelihood Estimation\n",
    "\n",
    "* Maximum likelihood estimation (MLE) is a technique used for estimating the parameters of a given distribution, using some observed data. For example, if a population is known to follow a “normal distribution” but the “mean” and “variance” are unknown, MLE can be used to estimate them using a limited sample of the population\n",
    "\n",
    "* likelihood expression is in the form of: L(parameters | data ). Meaning of this is, “likelihood of having these parameters, once the data are these”.\n",
    "* We talk about probability when we know the model parameters and when predicting a value from that model. So there we talk about how probable is the resultant value to be come out from that model. So probability is: P(data | parameters)\n",
    "\n",
    "So let’s define our linear model that needed to be estimated as ŷ.\n",
    "\n",
    "$ \\hat{y}=w_{0}+w_{1} x_{1}+\\ldots+w_{d} x_{d} $\n",
    "Mean squared error (MSE)\n",
    "\n",
    "To calculate the MSE, we take the difference between model’s predictions and the ground truth, square it, and average it out across the whole dataset.\n",
    "\n",
    "** Here the equation is **y=Mx+B**, where M is the slope of the line and B is y-intercept of the line.\n",
    " We want to find M (slope ) and B (y-intercept) that minimizes the squared error!\n",
    " equation that will give us the mean squared error for all our points.**\n",
    "\n",
    "$\\mathbf{M S E}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\tilde{y}_{i}\\right)^{2}$\n",
    "\n",
    "Let’s take each point on the graph, and we’ll do our calculation (y-y’)².\n",
    "But what is y’, and how do we calculate it? We do not have it as part of the data.in order to calculate y’, we need to use our line equation, y=mx+b, and put the x in the equation.\n",
    "\n",
    "$M S E=\\left(y_{1}-\\left(m x_{1}+b\\right)\\right)^{2}+\\left(y_{2}-\\left(m x_{2}+b\\right)\\right)^{2}+\\ldots+\\left(y_{n}-\\left(m x_{n}+b\\right)\\right)^{2}$\n",
    "\n",
    "Equations for slope and y-intercept\n",
    "$m=\\frac{\\overline{x y}-\\overline{x} \\overline{y}}{\\overline{x^{2}}-(\\overline{x})^{2}} \\quad b=\\overline{y}-m \\overline{x}$\n",
    "\n",
    "Let’s take 3 points, (1,2), (2,1), (4,3)\n",
    "Let’s find M and B for the equation y=mx+b.\n",
    "\n",
    "$\\overline{x}=\\frac{1+2+4}{3}=\\frac{7}{3}$\n",
    "\n",
    "$\\overline{y}=\\frac{2+1+3}{3}=2$\n",
    "\n",
    "$\\overline{x y}=\\frac{1 \\cdot 2+2 \\cdot 1+4 \\cdot 3}{3}=\\frac{16}{3}$\n",
    "\n",
    "$\\overline{x^{2}}=\\frac{1^{2}+2^{2}+4^{2}}{3}=\\frac{21}{3}=7$\n",
    "\n",
    "$m=\\frac{\\frac{7}{3} \\cdot 2-\\frac{16}{3}}{\\left(\\frac{7}{3}\\right)^{2}-7}=\\frac{\\frac{14}{3}-\\frac{16}{3}}{\\frac{49}{9}-7}=\\frac{-\\frac{2}{3}}{-\\frac{14}{9}}=\\frac{3}{7}$\n",
    "\n",
    "$b=2-\\frac{3}{7} \\cdot \\frac{7}{3}=2-1=1$\n",
    "\n",
    "$y=\\frac{3}{7} x+1$\n",
    "\n",
    "\n",
    "the line passes through the lines in such a way that it minimizes the squared distances.\n",
    "\n",
    "<img src='https://cdn-media-1.freecodecamp.org/images/DlKy-Eekc0SdHpcOeQPGJobo7jYLfTh0pI8Q' width='30%' height='30%'/>\n",
    "\n",
    "Drawbacks of MSE \n",
    "\n",
    "Disadvantage: \n",
    "* If our model makes a single very bad prediction, the squaring part of the function magnifies the error. \n",
    "* Yet in many practical cases we don’t care much about these outliers and are aiming for more of a well-rounded model that performs good enough on the majority.\n",
    "* the MSE as the cost function for logistic regression is because you don't want your cost function to be non-convex in nature. If the cost function is not convex then its difficult for the function to optimally converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Can gradient descent be used to find the parameters for linear regression? \n",
    "\n",
    "gradient descent can be applied to solve a linear regression problem. While the model in our example was a line, the concept of minimizing a cost function to tune parameters also applies to regression problems that use higher order polynomials and other problems found around the machine learning world.\n",
    "We used gradient descent to iteratively estimate m and b, however we could have also solved for them directly.\n",
    "\n",
    "some snapshots of gradient descent running for 2000 iterations for our example problem. We start out at point m = -1 b = 0. Each iteration m and b are updated to values that yield slightly lower error than the previous iteration. The left plot displays the current location of the gradient descent search (blue dot) and the path taken to get there (black line). The right plot displays the corresponding line for the current search location. Eventually we ended up with a pretty accurate fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://spin.atomicobject.com/wp-content/uploads/gradient_descent_search1.png\" alt=\"drawing\" width=\"50%\" height=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the concept of minimizing a cost function to tune parameters also applies to regression problems that use higher order polynomials\n",
    "\n",
    "Linear classification\n",
    " * A classification algorithm (Classifier) that makes its classification based on a linear\n",
    "    predictor function combining a set of weights with the feature vector\n",
    "    $y=f(\\vec{w} \\cdot \\vec{x})=f\\left(\\sum_{j} w_{j} x_{j}\\right)$\n",
    " * Decision boundaries is flat\n",
    " * Line, plane, ….\n",
    " * May involve non-linear operations\n",
    "\n",
    "**Explicitly creating the discriminant function (Discriminant function)** \n",
    "   * Perceptron\n",
    "   * Support vector machine\n",
    "   \n",
    "**Probabilistic approach**\n",
    "   * Model the posterior distribution\n",
    "   * Algorithms\n",
    "       * Logistic regression\n",
    "\n",
    "Relation between MLE and MSE for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i2.wp.com/www.jessicayung.com/wp-content/uploads/2018/06/mse-ml.png?w=1452' width= 30% height=30%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that minimising the mean-squared error (MSE) is not just something vaguely intuitive, but emerges from maximising the likelihood on a linear Gaussian model.\n",
    "the data is described by the linear model \n",
    "$\\mathbf{y}=\\mathbf{w} \\mathbf{X}+\\epsilon,$ where $\\epsilon_{i} \\sim N\\left(\\epsilon_{i} ; 0, \\sigma_{e}^{2}\\right)$\n",
    "\n",
    "\n",
    "Assume \\sigma^2_e is known and the datapoints are i.i.d. (independent and identically distributed).\n",
    "\n",
    "Note: the notation $N\\left(\\epsilon_{i} ; 0, \\sigma_{e}^{2}\\right)$ means that we are describing the distribution of $\\epsilon_{i}$, and that it is distributed as \n",
    "$N\\left(0,\\sigma_{e}^{2}\\right)$\n",
    "\n",
    "<img src='https://i2.wp.com/www.jessicayung.com/wp-content/uploads/2018/06/linear-mse.png?resize=1024%2C688' width=30% height=30%/>\n",
    "\n",
    "**Proof:**\n",
    "The log likelihood of our model is\n",
    "$\\log p(\\mathbf{y} | \\mathbf{X}, \\mathbf{w})=\\sum_{i=1}^{N} \\log p\\left(y_{i} | \\mathbf{x}_{\\mathbf{i}}, \\theta\\right)$\n",
    "\n",
    "But since the noise $\\epsilon$ is Gaussian (i.e. normally distributed), the likelihood is just\n",
    "\n",
    "$\\begin{aligned} \\log p(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}) &=\\sum_{i=1}^{N} \\log N\\left(y_{i} ; \\mathbf{x}_{\\mathbf{i}} \\mathbf{w}, \\sigma^{2}\\right) \\\\ &=\\sum_{i=1}^{N} \\log \\frac{1}{\\sqrt{2 \\pi \\sigma_{e}^{2}}} \\exp \\left(-\\frac{\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2}}{2 \\sigma_{e}^{2}}\\right)^{2} \\\\ &=-\\frac{N}{2} \\log 2 \\pi \\sigma_{e}^{2}-\\sum_{i=1}^{N} \\frac{\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2}}{2 \\sigma_{e}^{2}} \\end{aligned}$\n",
    "\n",
    "where N is the number of datapoints.\n",
    "\n",
    "$\\begin{aligned} \\mathbf{w}_{M L E} &=\\arg \\max _{\\mathbf{w}}-\\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2} \\\\ &=\\arg \\min _{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2} \\\\ &=\\arg \\min _{\\mathbf{w}} \\mathrm{MSE}_{\\text { train }} \\end{aligned}$\n",
    "\n",
    "\n",
    "That is, the parameters \\mathbf{w} chosen to maximise the likelihood are exactly those chosen to minimise the mean-squared error.\n",
    "\n",
    "Bonus question: a: Sigmoid function symmetric proof\n",
    "\n",
    "The derivative of the sigmoid function\n",
    "The derivative itself has a very convenient and beautiful form:\n",
    "    $\\frac{d \\sigma(x)}{d x}=\\sigma(x) \\cdot(1-\\sigma(x))$\n",
    "    \n",
    " This means that it's very easy to compute the derivative of the sigmoid function if you've already calculated the sigmoid function itself. E.g. when backpropagating errors in a neural network through a layer of nodes with a sigmoid activation function,σ(x)has already been computed during the forward pass.\n",
    "\n",
    "<img src='https://hvidberrrg.github.io/deep_learning/activation_functions/assets/sigmoid_derivative.png' width= 30% height= 30%/>\n",
    "\n",
    "we'll first derive:\n",
    "$\\frac{d \\sigma(x)}{d x}=\\sigma(x) \\cdot \\sigma(-x)$ (7)\n",
    "\n",
    "Then equation \n",
    " follows directly from the above fact combined with equation \n",
    "which tells us that σ(−x)=1−σ(x)). So here goes:\n",
    "\n",
    "$\\begin{aligned} \\frac{d \\sigma(x)}{d x} &=\\frac{d}{d x}\\left(\\frac{1}{1+e^{-x}}\\right) \\\\ &=\\frac{d}{d x}\\left(1+e^{-x}\\right)^{-1} \\\\ &=-\\left(1+e^{-x}\\right)^{-2} \\cdot\\left(-e^{-x}\\right) \\\\ &=\\frac{e^{-x}}{\\left(1+e^{-x}\\right)^{2}} \\\\ &=\\frac{1}{1+e^{-x}} \\cdot \\frac{e^{-x}}{1+e^{-x}} \\\\ &=\\sigma(x) \\cdot \\sigma(-x) \\end{aligned}$\n",
    "\n",
    "Where the last equality follows directly from equation \n",
    "(1)(please refer to the margin note for (1), for the alternate form of the sigmoid function).\n",
    "As should be evident from the graph of the derivative of the sigmoid function it's symmetric across the vertical axis, that is:\n",
    "\n",
    "$\\frac{d \\sigma(x)}{d x}=\\frac{d \\sigma(-x)}{d x}$ (8)\n",
    "\n",
    "This can also easily be seen from equation (7), as shown by the following:\n",
    "    \n",
    "$\\frac{d \\sigma(x)}{d x}=\\sigma(x) \\cdot \\sigma(-x)=\\sigma(-x) \\cdot \\sigma(-(-x))=\\frac{d \\sigma(-x)}{d x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
