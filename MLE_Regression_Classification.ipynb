{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Given a matrix equation, the normal equation is that which minimizes the sum of the square differences between the left and right sides: it is called the normal equation because it is normal to the range of the original linear equation. https://stats.stackexchange.com/questions/305720/why-do-we-call-the-equations-of-least-square-estimation-in-linear-regression-the \n",
    "Equations of least square estimation in linear regression are known as \"normal equations\". This is because of what the output values of the original inputs represent. In essence, the outputs from the least squares equations reduces the range from its entirety to only the differences. We can think of this in terms of geometry where the square root of the sum of two sides of which each were squared gives the length of the hypotenuse in a right triangle. This means that we are finding the perpendicular difference between outputs vs expected outputs.\n",
    "8. Feature scaling is not absolutely needed for linear regression. However, without it, it is possible that some inintended consequences may occur such as non-convergence and extremely inefficient processes.\n",
    "9. Given that logistic regression is used for classification of dependent variables, maximizing the likelihood that the hypothesis will accurately classify previously unseen inputs means restricting the outputs from infintely many possibilities to a finite set. With that in mind, what follows is the mathematical progression from initial logistic regression to the max likelihood formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{array}{c}{P\\left(Y_{i}=\\operatorname{cat} | x_{i} ; \\theta\\right)=\\theta^{T} \\overline{x}} \\\\ {\\theta^{T}=\\left[\\theta_{0}, \\theta_{1}, \\ldots, \\theta_{n}\\right] \\overline{x}=\\left[1, x_{0}, x_{1}, \\ldots, x_{n}\\right]} \\\\ {h(x)=\\sigma\\left(\\theta^{T} \\overline{x}\\right)=\\frac{1}{1+e^{-\\theta^{T} x}}} \\\\ {P\\left(Y_{i}=\\operatorname{cat} | x_{i} ; \\theta\\right)=\\frac{1}{1-e^{-\\theta^{T} x}}} \\\\ {P\\left(Y_{i}=\\operatorname{dog} | x_{i} ; \\theta\\right)=1-h(x)}\\end{array}\n",
       "\\begin{array}{c}{P\\left(Y_{i} | x_{i} ; \\theta\\right)=h\\left(x_{i}\\right)^{Y_{i}}\\left(1-h\\left(x_{i}\\right)\\right)^{1-Y_{i}}(\\text {Bernoulli Distribution})} \\\\ {L(\\theta)=P(Y | X ; \\theta)=\\prod_{i=1}^{m} h\\left(x_{i}\\right)^{Y_{i}} |(X, \\overline{Y})} \\\\ {\\ell(\\theta)=\\sum_{i=1}^{m} Y_{i} \\log \\left(\\sigma\\left(\\theta^{T} \\overline{x_{i}}\\right)\\right)+\\left(1-Y_{i}\\right)\\left(1-\\sigma\\left(\\theta^{T} x_{i}\\right)\\right)}\\end{array}\n",
       "\\begin{array}{c}{\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}=\\frac{y}{\\sigma\\left(\\theta^{T} \\overline{x}\\right)} \\frac{\\partial \\sigma\\left(\\theta^{T} \\overline{x}\\right)}{\\partial \\theta}+\\frac{(1-y)}{1-\\sigma\\left(\\theta^{T} \\overline{x}\\right)} \\frac{-\\partial \\sigma\\left(\\theta^{T} \\overline{x}\\right)}{\\partial \\theta}} \\\\ {\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}=(y-h(x)) \\overline{x}} \\\\ {\\operatorname{Max} \\text {Log Likelihood}=\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}=\\left(y-\\sigma\\left(\\theta^{T} \\overline{x}\\right)\\right) \\overline{x}}\\end{array}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{array}{c}{P\\left(Y_{i}=\\operatorname{cat} | x_{i} ; \\theta\\right)=\\theta^{T} \\overline{x}} \\\\ {\\theta^{T}=\\left[\\theta_{0}, \\theta_{1}, \\ldots, \\theta_{n}\\right] \\overline{x}=\\left[1, x_{0}, x_{1}, \\ldots, x_{n}\\right]} \\\\ {h(x)=\\sigma\\left(\\theta^{T} \\overline{x}\\right)=\\frac{1}{1+e^{-\\theta^{T} x}}} \\\\ {P\\left(Y_{i}=\\operatorname{cat} | x_{i} ; \\theta\\right)=\\frac{1}{1-e^{-\\theta^{T} x}}} \\\\ {P\\left(Y_{i}=\\operatorname{dog} | x_{i} ; \\theta\\right)=1-h(x)}\\end{array}\n",
    "\\begin{array}{c}{P\\left(Y_{i} | x_{i} ; \\theta\\right)=h\\left(x_{i}\\right)^{Y_{i}}\\left(1-h\\left(x_{i}\\right)\\right)^{1-Y_{i}}(\\text {Bernoulli Distribution})} \\\\ {L(\\theta)=P(Y | X ; \\theta)=\\prod_{i=1}^{m} h\\left(x_{i}\\right)^{Y_{i}} |(X, \\overline{Y})} \\\\ {\\ell(\\theta)=\\sum_{i=1}^{m} Y_{i} \\log \\left(\\sigma\\left(\\theta^{T} \\overline{x_{i}}\\right)\\right)+\\left(1-Y_{i}\\right)\\left(1-\\sigma\\left(\\theta^{T} x_{i}\\right)\\right)}\\end{array}\n",
    "\\begin{array}{c}{\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}=\\frac{y}{\\sigma\\left(\\theta^{T} \\overline{x}\\right)} \\frac{\\partial \\sigma\\left(\\theta^{T} \\overline{x}\\right)}{\\partial \\theta}+\\frac{(1-y)}{1-\\sigma\\left(\\theta^{T} \\overline{x}\\right)} \\frac{-\\partial \\sigma\\left(\\theta^{T} \\overline{x}\\right)}{\\partial \\theta}} \\\\ {\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}=(y-h(x)) \\overline{x}} \\\\ {\\operatorname{Max} \\text {Log Likelihood}=\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}=\\left(y-\\sigma\\left(\\theta^{T} \\overline{x}\\right)\\right) \\overline{x}}\\end{array}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Binary cross-entropy categorizes something as either True or False. Given that the outputs of the probability functions will be in the range (0, 1) with P > 0.5 == 1 and P < 0.5 == 0, one can see the correlation between the two concepts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
