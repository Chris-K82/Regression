{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***Question 0***\n",
    "\n",
    "What is a likelihood function? Also add a formula and explain what it means.\n",
    "\n",
    "Likelihood function often simply called Likelihood. Likelihood is the probability that an event that has already occurred would yield specific outcome. Probability refers to the occurrence of the future events, while likelihood refers to as events with known outcomes. \n",
    "\n",
    "\n",
    "Formula:\n",
    "Let ‘X’ be a discrete random variable with probability function mass ‘p’ depending on a parameter ‘θ’. Then the function:\n",
    "\n",
    "$\\mathcal{L}(\\theta | x)=p_{\\theta}(x)=P_{\\theta}(X=x)$\n",
    "\n",
    "\n",
    "Considered as a function of ‘θ’ is the likelihood function, given the outcome ‘x’ of the random variable ‘X’.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 1***\n",
    "\n",
    "What is Maximum Likelihood estimation (MLE)? Can you give an example?\n",
    "\n",
    "Maximum likelihood estimation (MLE) is a method of estimating the parameters of a distribution by maximizing a likelihood function, so that under the assumed statistical model the observed data is most probable. \n",
    "The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate.\n",
    "\n",
    "\n",
    "Example\n",
    "Suppose that there is a bag containing 3 balls. Each ball is either red or blue, but we have no information in addition to this. Thus, the number of blue balls ‘θ’, might be 0, 1, 2, or 3. I am allowed to choose 4 balls at random from the bag with replacement. We define the random X1, X2, X3 and X4 as follows\n",
    "\n",
    "Xi = {  1      if the ith chosen ball is blue; and\n",
    "\n",
    "       0       if the ith chosen ball is red   }\n",
    "\n",
    "Note that Xi’s are i.i.d. which means independent and identically distributed and Xi ~ Bernoulli (θ/3). After doing the experiment, it is observed that  Xi’s:\n",
    "\n",
    "\n",
    "x1= 1, x= 0, x3 = 1, x4 = 1. \n",
    "Thus, I observe 3 blue balls and 1 red ball.\n",
    "1.\tFor each possible value of ‘θ’ find the probability of the observed sample, (x1,x2,x3,x4) =  (1,0,1,1).\n",
    "2.\tFor which value of ‘θ’ is the probability of the observed sample is the largest?\n",
    "\n",
    "\n",
    "Since Xi ~ Bernoulli (θ/3), we have\n",
    "\n",
    "\n",
    "PXi(x) =   {   θ/3                   for x=1\n",
    "\n",
    "           1 – θ/3                   for x = 0}\n",
    "\n",
    "Since Xi’s are independent, the joint PMF of X1, X2, X3 and X4 can be written as\n",
    "\n",
    "Px1,x2,x3,x4(x1,x2,x3,x4)  = (θ/3). (1- θ/3) (θ/3). (θ/3)\n",
    "                                                = (θ/3)^3. (1- θ/3)\n",
    "\n",
    "Note that PMF depends on θ, so we write as Px1,x2,x3,x4(x1,x2,x3,x4; θ ).\n",
    "We obtain the values for the probability (1,0,1,1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The probability of observed sample for θ =0 and θ =3 is zero. This makes sense because our sample included both red and blue balls. From the table we see that the probability of the observed data is maximized for θ =2. This means that the observed data is most likely to occur for θ =2. For this reason, we may choose θ ^=2 as our estimate of θ. This is called the maximum likelihood estimate (MLE) of θ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 2***\n",
    "\n",
    "\n",
    "How is linear regression related to Pytorch and gradient descent?\n",
    " \n",
    "Linear regression using Pytorch\n",
    "Linear regression is a statistical method to study the relationship between two continuous variables.  Linear regression can be used using Pytorch. \n",
    "\n",
    "\n",
    "\n",
    "The basic steps involves importing necessary libraries \n",
    "\n",
    "The basic steps involves importing necessary libraries \n",
    "•\tImporting necessary libraries\n",
    "\n",
    "•\tGiving data, e.g. dependent and independent variables\n",
    "\n",
    "•\tDefining the model\n",
    "\n",
    "   1-Initializing model\n",
    "   \n",
    "   2-Declaring the forward pass\n",
    "   \n",
    "• Select optimizer and loss criteria\n",
    "\n",
    "   1-We can use loss function as MSE\n",
    "   \n",
    "   2-Also,  we can use Stochastic gradient descent as optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 3***\n",
    "\n",
    "Write out MSE (Mean square error) loss for linear regression. \n",
    "\n",
    "Mean Square Error is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values.\n",
    "\n",
    "$M S E=\\frac{\\sum_{i=1}^{n}\\left(y_{i}-y_{i}^{p}\\right)^{2}}{n}$\n",
    "\n",
    "\n",
    "Below is a plot of an MSE function where the true target value is 100, and the predicted values range between -10,000 to 10,000. The MSE loss (Y-axis) reaches its minimum value at prediction (X-axis) = 100. The range is 0 to ∞.\n",
    "\n",
    "![Alt text](https://miro.medium.com/max/576/1*EqTaoCB1NmJnsRYEezSACA.png)\n",
    "\n",
    "Could we also use this loss for classification?\n",
    "Yes, from the basics of statistical machine learning, the loss function is the negative-log-likelihood of the model. To find best parameters, we need to minimize this NLL loss. The MSE is the NLL for linear regression model (with Gaussian likelihood). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 4*** Relation between MLE and MSE for linear regression\n",
    "\n",
    "<img src='https://i2.wp.com/www.jessicayung.com/wp-content/uploads/2018/06/mse-ml.png?w=1452' width= 30% height=30%/>\n",
    "\n",
    "It shows that minimising the mean-squared error (MSE) is not just something vaguely intuitive, but emerges from maximising the likelihood on a linear Gaussian model.\n",
    "the data is described by the linear model \n",
    "$\\mathbf{y}=\\mathbf{w} \\mathbf{X}+\\epsilon,$ where $\\epsilon_{i} \\sim N\\left(\\epsilon_{i} ; 0, \\sigma_{e}^{2}\\right)$\n",
    "\n",
    "\n",
    "Assume \\sigma^2_e is known and the datapoints are i.i.d. (independent and identically distributed).\n",
    "\n",
    "Note: the notation $N\\left(\\epsilon_{i} ; 0, \\sigma_{e}^{2}\\right)$ means that we are describing the distribution of $\\epsilon_{i}$, and that it is distributed as \n",
    "$N\\left(0,\\sigma_{e}^{2}\\right)$\n",
    "\n",
    "<img src='https://i2.wp.com/www.jessicayung.com/wp-content/uploads/2018/06/linear-mse.png?resize=1024%2C688' width=30% height=30%/>\n",
    "\n",
    "**Proof:**\n",
    "The log likelihood of our model is\n",
    "$\\log p(\\mathbf{y} | \\mathbf{X}, \\mathbf{w})=\\sum_{i=1}^{N} \\log p\\left(y_{i} | \\mathbf{x}_{\\mathbf{i}}, \\theta\\right)$\n",
    "\n",
    "But since the noise $\\epsilon$ is Gaussian (i.e. normally distributed), the likelihood is just\n",
    "\n",
    "$\\begin{aligned} \\log p(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}) &=\\sum_{i=1}^{N} \\log N\\left(y_{i} ; \\mathbf{x}_{\\mathbf{i}} \\mathbf{w}, \\sigma^{2}\\right) \\\\ &=\\sum_{i=1}^{N} \\log \\frac{1}{\\sqrt{2 \\pi \\sigma_{e}^{2}}} \\exp \\left(-\\frac{\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2}}{2 \\sigma_{e}^{2}}\\right)^{2} \\\\ &=-\\frac{N}{2} \\log 2 \\pi \\sigma_{e}^{2}-\\sum_{i=1}^{N} \\frac{\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2}}{2 \\sigma_{e}^{2}} \\end{aligned}$\n",
    "\n",
    "where N is the number of datapoints.\n",
    "\n",
    "$\\begin{aligned} \\mathbf{w}_{M L E} &=\\arg \\max _{\\mathbf{w}}-\\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2} \\\\ &=\\arg \\min _{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2} \\\\ &=\\arg \\min _{\\mathbf{w}} \\mathrm{MSE}_{\\text { train }} \\end{aligned}$\n",
    "\n",
    "\n",
    "That is, the parameters \\mathbf{w} chosen to maximise the likelihood are exactly those chosen to minimise the mean-squared error.\n",
    "\n",
    "Bonus question: a: Sigmoid function symmetric proof\n",
    "\n",
    "The derivative of the sigmoid function\n",
    "The derivative itself has a very convenient and beautiful form:\n",
    "    $\\frac{d \\sigma(x)}{d x}=\\sigma(x) \\cdot(1-\\sigma(x))$\n",
    "    \n",
    " This means that it's very easy to compute the derivative of the sigmoid function if you've already calculated the sigmoid function itself. E.g. when backpropagating errors in a neural network through a layer of nodes with a sigmoid activation function,σ(x)has already been computed during the forward pass.\n",
    "\n",
    "<img src='https://hvidberrrg.github.io/deep_learning/activation_functions/assets/sigmoid_derivative.png' width= 30% height= 30%/>\n",
    "\n",
    "we'll first derive:\n",
    "$\\frac{d \\sigma(x)}{d x}=\\sigma(x) \\cdot \\sigma(-x)$ (7)\n",
    "\n",
    "Then equation \n",
    " follows directly from the above fact combined with equation \n",
    "which tells us that σ(−x)=1−σ(x)). So here goes:\n",
    "\n",
    "$\\begin{aligned} \\frac{d \\sigma(x)}{d x} &=\\frac{d}{d x}\\left(\\frac{1}{1+e^{-x}}\\right) \\\\ &=\\frac{d}{d x}\\left(1+e^{-x}\\right)^{-1} \\\\ &=-\\left(1+e^{-x}\\right)^{-2} \\cdot\\left(-e^{-x}\\right) \\\\ &=\\frac{e^{-x}}{\\left(1+e^{-x}\\right)^{2}} \\\\ &=\\frac{1}{1+e^{-x}} \\cdot \\frac{e^{-x}}{1+e^{-x}} \\\\ &=\\sigma(x) \\cdot \\sigma(-x) \\end{aligned}$\n",
    "\n",
    "Where the last equality follows directly from equation \n",
    "(1)(please refer to the margin note for (1), for the alternate form of the sigmoid function).\n",
    "As should be evident from the graph of the derivative of the sigmoid function it's symmetric across the vertical axis, that is:\n",
    "\n",
    "$\\frac{d \\sigma(x)}{d x}=\\frac{d \\sigma(-x)}{d x}$ (8)\n",
    "\n",
    "This can also easily be seen from equation (7), as shown by the following:\n",
    "    \n",
    "$\\frac{d \\sigma(x)}{d x}=\\sigma(x) \\cdot \\sigma(-x)=\\sigma(-x) \\cdot \\sigma(-(-x))=\\frac{d \\sigma(-x)}{d x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 5***\n",
    "\n",
    "* In linear regression the trick that we do is, we take the model that we need to find, as the mean of the above stated normal distribution. Because we know how to find MLE values of a mean in a normal distribution.\n",
    "\n",
    "\n",
    "Maximum Likelihood Estimation\n",
    "\n",
    "* Maximum likelihood estimation (MLE) is a technique used for estimating the parameters of a given distribution, using some observed data. For example, if a population is known to follow a “normal distribution” but the “mean” and “variance” are unknown, MLE can be used to estimate them using a limited sample of the population\n",
    "\n",
    "* likelihood expression is in the form of: L(parameters | data ). Meaning of this is, “likelihood of having these parameters, once the data are these”.\n",
    "* We talk about probability when we know the model parameters and when predicting a value from that model. So there we talk about how probable is the resultant value to be come out from that model. So probability is: P(data | parameters)\n",
    "\n",
    "So let’s define our linear model that needed to be estimated as ŷ.\n",
    "\n",
    "$ \\hat{y}=w_{0}+w_{1} x_{1}+\\ldots+w_{d} x_{d} $\n",
    "Mean squared error (MSE)\n",
    "\n",
    "To calculate the MSE, we take the difference between model’s predictions and the ground truth, square it, and average it out across the whole dataset.\n",
    "\n",
    "** Here the equation is **y=Mx+B**, where M is the slope of the line and B is y-intercept of the line.\n",
    " We want to find M (slope ) and B (y-intercept) that minimizes the squared error!\n",
    " equation that will give us the mean squared error for all our points.**\n",
    "\n",
    "$\\mathbf{M S E}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\tilde{y}_{i}\\right)^{2}$\n",
    "\n",
    "Let’s take each point on the graph, and we’ll do our calculation (y-y’)².\n",
    "But what is y’, and how do we calculate it? We do not have it as part of the data.in order to calculate y’, we need to use our line equation, y=mx+b, and put the x in the equation.\n",
    "\n",
    "$M S E=\\left(y_{1}-\\left(m x_{1}+b\\right)\\right)^{2}+\\left(y_{2}-\\left(m x_{2}+b\\right)\\right)^{2}+\\ldots+\\left(y_{n}-\\left(m x_{n}+b\\right)\\right)^{2}$\n",
    "\n",
    "Equations for slope and y-intercept\n",
    "$m=\\frac{\\overline{x y}-\\overline{x} \\overline{y}}{\\overline{x^{2}}-(\\overline{x})^{2}} \\quad b=\\overline{y}-m \\overline{x}$\n",
    "\n",
    "Let’s take 3 points, (1,2), (2,1), (4,3)\n",
    "Let’s find M and B for the equation y=mx+b.\n",
    "\n",
    "$\\overline{x}=\\frac{1+2+4}{3}=\\frac{7}{3}$\n",
    "\n",
    "$\\overline{y}=\\frac{2+1+3}{3}=2$\n",
    "\n",
    "$\\overline{x y}=\\frac{1 \\cdot 2+2 \\cdot 1+4 \\cdot 3}{3}=\\frac{16}{3}$\n",
    "\n",
    "$\\overline{x^{2}}=\\frac{1^{2}+2^{2}+4^{2}}{3}=\\frac{21}{3}=7$\n",
    "\n",
    "$m=\\frac{\\frac{7}{3} \\cdot 2-\\frac{16}{3}}{\\left(\\frac{7}{3}\\right)^{2}-7}=\\frac{\\frac{14}{3}-\\frac{16}{3}}{\\frac{49}{9}-7}=\\frac{-\\frac{2}{3}}{-\\frac{14}{9}}=\\frac{3}{7}$\n",
    "\n",
    "$b=2-\\frac{3}{7} \\cdot \\frac{7}{3}=2-1=1$\n",
    "\n",
    "$y=\\frac{3}{7} x+1$\n",
    "\n",
    "\n",
    "the line passes through the lines in such a way that it minimizes the squared distances.\n",
    "\n",
    "<img src='https://cdn-media-1.freecodecamp.org/images/DlKy-Eekc0SdHpcOeQPGJobo7jYLfTh0pI8Q' width='30%' height='30%'/>\n",
    "\n",
    "Drawbacks of MSE \n",
    "\n",
    "Disadvantage: \n",
    "* If our model makes a single very bad prediction, the squaring part of the function magnifies the error. \n",
    "* Yet in many practical cases we don’t care much about these outliers and are aiming for more of a well-rounded model that performs good enough on the majority.\n",
    "* the MSE as the cost function for logistic regression is because you don't want your cost function to be non-convex in nature. If the cost function is not convex then its difficult for the function to optimally converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 6*** Can gradient descent be used to find the parameters for linear regression? \n",
    "\n",
    "gradient descent can be applied to solve a linear regression problem. While the model in our example was a line, the concept of minimizing a cost function to tune parameters also applies to regression problems that use higher order polynomials and other problems found around the machine learning world.\n",
    "We used gradient descent to iteratively estimate m and b, however we could have also solved for them directly.\n",
    "\n",
    "some snapshots of gradient descent running for 2000 iterations for our example problem. We start out at point m = -1 b = 0. Each iteration m and b are updated to values that yield slightly lower error than the previous iteration. The left plot displays the current location of the gradient descent search (blue dot) and the path taken to get there (black line). The right plot displays the corresponding line for the current search location. Eventually we ended up with a pretty accurate fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://spin.atomicobject.com/wp-content/uploads/gradient_descent_search1.png\" alt=\"drawing\" width=\"50%\" height=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the concept of minimizing a cost function to tune parameters also applies to regression problems that use higher order polynomials\n",
    "\n",
    "Linear classification\n",
    " * A classification algorithm (Classifier) that makes its classification based on a linear\n",
    "    predictor function combining a set of weights with the feature vector\n",
    "    $y=f(\\vec{w} \\cdot \\vec{x})=f\\left(\\sum_{j} w_{j} x_{j}\\right)$\n",
    " * Decision boundaries is flat\n",
    " * Line, plane, ….\n",
    " * May involve non-linear operations\n",
    "\n",
    "**Explicitly creating the discriminant function (Discriminant function)** \n",
    "   * Perceptron\n",
    "   * Support vector machine\n",
    "   \n",
    "**Probabilistic approach**\n",
    "   * Model the posterior distribution\n",
    "   * Algorithms\n",
    "       * Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i2.wp.com/www.jessicayung.com/wp-content/uploads/2018/06/mse-ml.png?w=1452' width= 30% height=30%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Given a matrix equation, the normal equation is that which minimizes the sum of the square differences between the left and right sides: it is called the normal equation because it is normal to the range of the original linear equation. https://stats.stackexchange.com/questions/305720/why-do-we-call-the-equations-of-least-square-estimation-in-linear-regression-the \n",
    "Equations of least square estimation in linear regression are known as \"normal equations\". This is because of what the output values of the original inputs represent. In essence, the outputs from the least squares equations reduces the range from its entirety to only the differences. We can think of this in terms of geometry where the square root of the sum of two sides of which each were squared gives the length of the hypotenuse in a right triangle. This means that we are finding the perpendicular difference between outputs vs expected outputs.\n",
    "8. Feature scaling is not absolutely needed for linear regression. However, without it, it is possible that some inintended consequences may occur such as non-convergence and extremely inefficient processes.\n",
    "9. Given that logistic regression is used for classification of dependent variables, maximizing the likelihood that the hypothesis will accurately classify previously unseen inputs means restricting the outputs from infintely many possibilities to a finite set. With that in mind, what follows is the mathematical progression from initial logistic regression to the max likelihood formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{array}{c}{P\\left(Y_{i}=\\operatorname{cat} | x_{i} ; \\theta\\right)=\\theta^{T} \\overline{x}} \\\\ {\\theta^{T}=\\left[\\theta_{0}, \\theta_{1}, \\ldots, \\theta_{n}\\right] \\overline{x}=\\left[1, x_{0}, x_{1}, \\ldots, x_{n}\\right]} \\\\ {h(x)=\\sigma\\left(\\theta^{T} \\overline{x}\\right)=\\frac{1}{1+e^{-\\theta^{T} x}}} \\\\ {P\\left(Y_{i}=\\operatorname{cat} | x_{i} ; \\theta\\right)=\\frac{1}{1-e^{-\\theta^{T} x}}} \\\\ {P\\left(Y_{i}=\\operatorname{dog} | x_{i} ; \\theta\\right)=1-h(x)}\\end{array}\n",
       "\\begin{array}{c}{P\\left(Y_{i} | x_{i} ; \\theta\\right)=h\\left(x_{i}\\right)^{Y_{i}}\\left(1-h\\left(x_{i}\\right)\\right)^{1-Y_{i}}(\\text {Bernoulli Distribution})} \\\\ {L(\\theta)=P(Y | X ; \\theta)=\\prod_{i=1}^{m} h\\left(x_{i}\\right)^{Y_{i}} |(X, \\overline{Y})} \\\\ {\\ell(\\theta)=\\sum_{i=1}^{m} Y_{i} \\log \\left(\\sigma\\left(\\theta^{T} \\overline{x_{i}}\\right)\\right)+\\left(1-Y_{i}\\right)\\left(1-\\sigma\\left(\\theta^{T} x_{i}\\right)\\right)}\\end{array}\n",
       "\\begin{array}{c}{\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}=\\frac{y}{\\sigma\\left(\\theta^{T} \\overline{x}\\right)} \\frac{\\partial \\sigma\\left(\\theta^{T} \\overline{x}\\right)}{\\partial \\theta}+\\frac{(1-y)}{1-\\sigma\\left(\\theta^{T} \\overline{x}\\right)} \\frac{-\\partial \\sigma\\left(\\theta^{T} \\overline{x}\\right)}{\\partial \\theta}} \\\\ {\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}=(y-h(x)) \\overline{x}} \\\\ {\\operatorname{Max} \\text {Log Likelihood}=\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}=\\left(y-\\sigma\\left(\\theta^{T} \\overline{x}\\right)\\right) \\overline{x}}\\end{array}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{array}{c}{P\\left(Y_{i}=\\operatorname{cat} | x_{i} ; \\theta\\right)=\\theta^{T} \\overline{x}} \\\\ {\\theta^{T}=\\left[\\theta_{0}, \\theta_{1}, \\ldots, \\theta_{n}\\right] \\overline{x}=\\left[1, x_{0}, x_{1}, \\ldots, x_{n}\\right]} \\\\ {h(x)=\\sigma\\left(\\theta^{T} \\overline{x}\\right)=\\frac{1}{1+e^{-\\theta^{T} x}}} \\\\ {P\\left(Y_{i}=\\operatorname{cat} | x_{i} ; \\theta\\right)=\\frac{1}{1-e^{-\\theta^{T} x}}} \\\\ {P\\left(Y_{i}=\\operatorname{dog} | x_{i} ; \\theta\\right)=1-h(x)}\\end{array}\n",
    "\\begin{array}{c}{P\\left(Y_{i} | x_{i} ; \\theta\\right)=h\\left(x_{i}\\right)^{Y_{i}}\\left(1-h\\left(x_{i}\\right)\\right)^{1-Y_{i}}(\\text {Bernoulli Distribution})} \\\\ {L(\\theta)=P(Y | X ; \\theta)=\\prod_{i=1}^{m} h\\left(x_{i}\\right)^{Y_{i}} |(X, \\overline{Y})} \\\\ {\\ell(\\theta)=\\sum_{i=1}^{m} Y_{i} \\log \\left(\\sigma\\left(\\theta^{T} \\overline{x_{i}}\\right)\\right)+\\left(1-Y_{i}\\right)\\left(1-\\sigma\\left(\\theta^{T} x_{i}\\right)\\right)}\\end{array}\n",
    "\\begin{array}{c}{\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}=\\frac{y}{\\sigma\\left(\\theta^{T} \\overline{x}\\right)} \\frac{\\partial \\sigma\\left(\\theta^{T} \\overline{x}\\right)}{\\partial \\theta}+\\frac{(1-y)}{1-\\sigma\\left(\\theta^{T} \\overline{x}\\right)} \\frac{-\\partial \\sigma\\left(\\theta^{T} \\overline{x}\\right)}{\\partial \\theta}} \\\\ {\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}=(y-h(x)) \\overline{x}} \\\\ {\\operatorname{Max} \\text {Log Likelihood}=\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}=\\left(y-\\sigma\\left(\\theta^{T} \\overline{x}\\right)\\right) \\overline{x}}\\end{array}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Binary cross-entropy categorizes something as either True or False. Given that the outputs of the probability functions will be in the range (0, 1) with P > 0.5 == 1 and P < 0.5 == 0, one can see the correlation between the two concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. From an algebraic perspective, a function is symmetric if g(x) = -g(x) or if g(-x) = \n",
    "-g(-x). However, in the geometric sense, a function is symmetric if it can be rotated about a point on its graph and look the same as it did before the rotation. The sigmoid function has point symmetry about the point (0, 1/2). When rotated 180 degrees, it looks the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f01b591b860>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import section\n",
    "from matplotlib import pylab\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "#sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "# generate an Array with value ???\n",
    "# linespace generate an array from start and stop value\n",
    "# with requested number of elements. Example 10 elements or 100 elements.\n",
    "# \n",
    "x = plt.linspace(-10,10,10)\n",
    "y = plt.linspace(-10,10,100)\n",
    "\n",
    "# prepare the plot, associate the color r(ed) or b(lue) and the label \n",
    "plt.plot(x, sigmoid(x), 'r', label='linspace(-10,10,10)')\n",
    "plt.plot(y, sigmoid(y), 'b', label='linspace(-10,10,100)')\n",
    "\n",
    "# Draw the grid line in background.\n",
    "plt.grid()\n",
    "\n",
    "# Title & Subtitle\n",
    "plt.title('Sigmoid Function')\n",
    "plt.suptitle('Sigmoid')\n",
    "\n",
    "# place the legen boc in bottom right of the graph\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "# write the Sigmoid formula\n",
    "plt.text(4, 0.8, r'$\\sigma(x)=\\frac{1}{1+e^{-x}}$', fontsize=15)\n",
    "\n",
    "#resize the X and Y axes\n",
    "plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "plt.gca().yaxis.set_major_locator(plt.MultipleLocator(0.1))\n",
    " \n",
    "\n",
    "# plt.plot(x)\n",
    "plt.xlabel('X Axis')\n",
    "plt.ylabel('Y Axis')\n",
    "\n",
    "# prepare the plot, associate the color r(ed) or b(lue) and the label \n",
    "plt.plot(x, sigmoid(-1*x), 'r', label='linspace(-10,10,10)')\n",
    "plt.plot(y, sigmoid(-1*y), 'b', label='linspace(-10,10,100)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{array}{c}{\\sigma=\\left(y=\\frac{1}{1+e^{-x}}\\right)} \\\\ {\\frac{d y}{d x}=[1-y(x)] y(x)} \\\\ {\\frac{e^{-x}}{\\left(1+e^{-x}\\right)^{2}}} \\\\ {\\int y d x=x+\\ln \\left(1+e^{-x}\\right)}\\end{array}\n",
       "\\begin{array}{c}{=\\ln \\left(1+e^{x}\\right)} \\\\ {y^{\\prime}=\\ln \\left(1+e^{x}\\right)} \\\\ {x=\\ln \\left(1+e^{y}\\right)} \\\\ {e^{x}-1=e^{y}} \\\\ {\\ln \\left(e^{x}-1\\right)=y^{\\prime}} \\\\ {y=x \\ln \\left(e^{x}-1\\right)-\\left(1+\\mathrm{e}^{x}\\right)(\\text {nverted Sigmoid})}\\end{array}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{array}{c}{\\sigma=\\left(y=\\frac{1}{1+e^{-x}}\\right)} \\\\ {\\frac{d y}{d x}=[1-y(x)] y(x)} \\\\ {\\frac{e^{-x}}{\\left(1+e^{-x}\\right)^{2}}} \\\\ {\\int y d x=x+\\ln \\left(1+e^{-x}\\right)}\\end{array}\n",
    "\\begin{array}{c}{=\\ln \\left(1+e^{x}\\right)} \\\\ {y^{\\prime}=\\ln \\left(1+e^{x}\\right)} \\\\ {x=\\ln \\left(1+e^{y}\\right)} \\\\ {e^{x}-1=e^{y}} \\\\ {\\ln \\left(e^{x}-1\\right)=y^{\\prime}} \\\\ {y=x \\ln \\left(e^{x}-1\\right)-\\left(1+\\mathrm{e}^{x}\\right)(\\text {nverted Sigmoid})}\\end{array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients of w1:\n",
      "\n",
      " [[-0.06157492 -0.10827408 -0.16603831 -0.29573038]\n",
      " [-0.12314984 -0.21654817 -0.33207663 -0.59146077]\n",
      " [-0.18472476 -0.32482225 -0.49811494 -0.88719115]]\n"
     ]
    }
   ],
   "source": [
    "inp_neuron = 3\n",
    "out_neuron = 1\n",
    "hidden_neuron = 4\n",
    "\n",
    "x = np.array([[1,2,3]])\n",
    "y = np.array([6])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# defining weight matrices\n",
    "w1 = np.random.rand(inp_neuron, hidden_neuron)\n",
    "w2 = np.random.rand(hidden_neuron, out_neuron)\n",
    "\n",
    "# calculating activations\n",
    "hid = sigmoid(np.matmul(x, w1))    \n",
    "out = np.matmul(hid, w2)           \n",
    "        \n",
    "# defining loss function    \n",
    "loss = ((y - out)**2)/2\n",
    "\n",
    "# calculating the gredients of loss w.r.t weights\n",
    "out_delta = (out - y)                         \n",
    "hid_error = np.matmul(out_delta, w2.T)\n",
    "hid_delta = hid_error*hid*(1-hid)                         \n",
    "grad_w2 = np.matmul(hid.T, out_delta)  \n",
    "grad_w1 = np.matmul(x.T, hid_delta) \n",
    "\n",
    "print(\"gradients of w1:\\n\\n\", grad_w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@shivajbd/how-to-validate-your-gradient-expression-for-a-neural-network-8284ede6272\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
